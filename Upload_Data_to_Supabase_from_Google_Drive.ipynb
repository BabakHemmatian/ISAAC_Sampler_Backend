{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIYYp72krY4s",
        "outputId": "2bfe5ebb-cfe1-4094-b2c8-67bdda847d11"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.13' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "!pip install boto3 botocore asyncpg python-dotenv nest_asyncio pandas pyarrow s3fs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POBh3u-S0pEW",
        "outputId": "162e4829-235d-4d23-b6ec-67790bb7ad01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.40.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore\n",
            "  Downloading botocore-1.40.57-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.17.0)\n",
            "Downloading boto3-1.40.57-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.57-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.40.57 botocore-1.40.57 jmespath-1.0.1 s3transfer-0.14.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "\n",
        "# Set environment variables directly\n",
        "os.environ[\"SUPABASE_DB_URL\"] = \"postgresql://postgres.rfyavjpuqoepfkxhtzie:uiucReddit2025@aws-1-us-east-2.pooler.supabase.com:5432/postgres\"\n",
        "os.environ[\"SUPABASE_S3_ENDPOINT\"] = \"https://rfyavjpuqoepfkxhtzie.storage.supabase.co/storage/v1/s3\"\n",
        "os.environ[\"SUPABASE_S3_REGION\"] = \"us-east-2\"\n",
        "os.environ[\"SUPABASE_S3_ACCESS_KEY_ID\"] = \"f51548d432ea32dfd9471934c08fe8f1\"\n",
        "os.environ[\"SUPABASE_S3_SECRET_ACCESS_KEY\"] = \"e5220d24a9d841ab80065a33c9d2d6b3f4071e30a052951de3b6680f3519bf2a\"\n",
        "os.environ[\"SUPABASE_BUCKET_NAME\"] = \"refiltered-files\"\n",
        "os.environ[\"CHUNK_ROWS\"] = \"80000\"\n",
        "os.environ[\"MAX_CONCURRENCY\"] = \"4\"\n",
        "os.environ[\"MAX_OBJECT_MB\"] = \"100\"\n",
        "\n",
        "# Supabase Storage configuration\n",
        "SUPABASE_S3_ENDPOINT = os.getenv(\"SUPABASE_S3_ENDPOINT\")\n",
        "SUPABASE_S3_REGION = os.getenv(\"SUPABASE_S3_REGION\", \"us-east-1\")\n",
        "SUPABASE_S3_ACCESS_KEY_ID = os.getenv(\"SUPABASE_S3_ACCESS_KEY_ID\")\n",
        "SUPABASE_S3_SECRET_ACCESS_KEY = os.getenv(\"SUPABASE_S3_SECRET_ACCESS_KEY\")\n",
        "SUPABASE_BUCKET = os.getenv(\"SUPABASE_BUCKET_NAME\")\n",
        "\n",
        "# Initialize S3 client for Supabase Storage\n",
        "s3_client = boto3.client(\n",
        "    's3',\n",
        "    endpoint_url=SUPABASE_S3_ENDPOINT,\n",
        "    aws_access_key_id=SUPABASE_S3_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=SUPABASE_S3_SECRET_ACCESS_KEY,\n",
        "    region_name=SUPABASE_S3_REGION\n",
        ")\n",
        "\n",
        "# List all folders in the bucket\n",
        "def list_folders_in_bucket():\n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(Bucket=SUPABASE_BUCKET, Delimiter='/')\n",
        "        folders = []\n",
        "        if 'CommonPrefixes' in response:\n",
        "            for obj in response['CommonPrefixes']:\n",
        "                folder_name = obj['Prefix'].rstrip('/')\n",
        "                folders.append(folder_name)\n",
        "        return folders\n",
        "    except ClientError as e:\n",
        "        print(f\"Error listing folders: {e}\")\n",
        "        return []\n",
        "\n",
        "# List folders\n",
        "folders = list_folders_in_bucket()\n",
        "print(\"Available folders in Supabase Storage:\")\n",
        "for folder in folders:\n",
        "    print(f\"  - {folder}\")\n",
        "\n",
        "# Target folders to process\n",
        "target_folders = [\"ability\", \"age\", \"filtered_keywords_adv\", \"race\", \"sexuality\", \"skin_tone\", \"weight\"]\n",
        "print(f\"\\nTarget folders to process: {target_folders}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TZ79FogrcOG",
        "outputId": "e01b5ad3-031e-44b6-d601-798d8a052988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available folders in Supabase Storage:\n",
            "  - ability\n",
            "  - age\n",
            "  - race\n",
            "  - sexuality\n",
            "  - skin_tone\n",
            "  - weight\n",
            "\n",
            "Target folders to process: ['ability', 'age', 'filtered_keywords_adv', 'race', 'sexuality', 'skin_tone', 'weight']\n"
          ]
        }
      ],
      "source": [
        "def list_csv_files_in_folder(folder_name):\n",
        "    \"\"\"List all CSV files in a specific folder\"\"\"\n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(\n",
        "            Bucket=SUPABASE_BUCKET,\n",
        "            Prefix=f\"{folder_name}/\"\n",
        "        )\n",
        "        csv_files = []\n",
        "        if 'Contents' in response:\n",
        "            for obj in response['Contents']:\n",
        "                key = obj['Key']\n",
        "                if key.endswith('.csv'):\n",
        "                    csv_files.append(key)\n",
        "        return csv_files\n",
        "    except ClientError as e:\n",
        "        print(f\"Error listing files in {folder_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Check each target folder\n",
        "for folder in target_folders:\n",
        "    csv_files = list_csv_files_in_folder(folder)\n",
        "    print(f\"{folder}: {len(csv_files)} CSV files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all CSV files from the target folders\n",
        "def get_all_csv_files_from_folders(folders):\n",
        "    \"\"\"Get all CSV files from specified folders in Supabase Storage\"\"\"\n",
        "    all_files = []\n",
        "    for folder in folders:\n",
        "        print(f\"üìÅ Scanning folder: {folder}\")\n",
        "        files = list_csv_files_in_folder(folder)\n",
        "        all_files.extend(files)\n",
        "        print(f\"   Found {len(files)} CSV files\")\n",
        "    return all_files\n",
        "\n",
        "# Get all CSV files from target folders\n",
        "all_csv_files = get_all_csv_files_from_folders(target_folders)\n",
        "print(f\"\\nüìä Total CSV files to process: {len(all_csv_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all files\n",
        "print(f\"\\nüöÄ Starting processing...\")\n",
        "print(f\"Processing {len(all_csv_files)} files from {len(target_folders)} folders\")\n",
        "\n",
        "await process_files_by_s3_keys(\n",
        "    S3_KEYS=all_csv_files,\n",
        "    SINGLE_GROUP=None,  # Will use folder name as group\n",
        "    CHUNK_ROWS=80000,\n",
        "    MAX_CONCURRENCY=4,\n",
        "    MAX_OBJECT_BYTES=100 * 1024 * 1024,  # 100MB\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "I5RcX9-jrnBj",
        "outputId": "cb4039bc-39a5-4488-ed2d-6b19b84b2b36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Scanning folder: ability\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'list_csv_files_in_folder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2015650654.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;31m# Get all CSV files from target folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m \u001b[0mall_csv_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_csv_files_from_folders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_folders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìä Total CSV files to process: {len(all_csv_files)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2015650654.py\u001b[0m in \u001b[0;36mget_all_csv_files_from_folders\u001b[0;34m(folders)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üìÅ Scanning folder: {folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_csv_files_in_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0mall_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Found {len(files)} CSV files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'list_csv_files_in_folder' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import sys\n",
        "import math\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Optional, List, Tuple, Dict\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import pandas as pd\n",
        "import asyncpg\n",
        "import boto3\n",
        "from boto3.s3.transfer import TransferConfig\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "SUPABASE_DB_URL = os.getenv(\"SUPABASE_DB_URL\")\n",
        "\n",
        "SUPABASE_S3_ENDPOINT = os.getenv(\"SUPABASE_S3_ENDPOINT\")\n",
        "SUPABASE_S3_REGION = os.getenv(\"SUPABASE_S3_REGION\", \"us-east-1\")\n",
        "SUPABASE_S3_ACCESS_KEY_ID = os.getenv(\"SUPABASE_S3_ACCESS_KEY_ID\")\n",
        "SUPABASE_S3_SECRET_ACCESS_KEY = os.getenv(\"SUPABASE_S3_SECRET_ACCESS_KEY\")\n",
        "SUPABASE_BUCKET = os.getenv(\"SUPABASE_BUCKET_NAME\")\n",
        "\n",
        "CHUNK_ROWS = int(os.getenv(\"CHUNK_ROWS\", \"80000\"))\n",
        "MAX_CONCURRENCY = int(os.getenv(\"MAX_CONCURRENCY\", \"4\"))\n",
        "MAX_OBJECT_MB = int(os.getenv(\"MAX_OBJECT_MB\", \"100\"))\n",
        "MAX_OBJECT_BYTES = MAX_OBJECT_MB * 1024 * 1024\n",
        "ROW_GROUP_ROWS = int(os.getenv(\"ROW_GROUP_ROWS\", \"128000\"))\n",
        "\n",
        "REQUIRED = [\n",
        "    (\"SUPABASE_DB_URL\", SUPABASE_DB_URL),\n",
        "    (\"SUPABASE_S3_ENDPOINT\", SUPABASE_S3_ENDPOINT),\n",
        "    (\"SUPABASE_S3_ACCESS_KEY_ID\", SUPABASE_S3_ACCESS_KEY_ID),\n",
        "    (\"SUPABASE_S3_SECRET_ACCESS_KEY\", SUPABASE_S3_SECRET_ACCESS_KEY),\n",
        "    (\"SUPABASE_BUCKET_NAME\", SUPABASE_BUCKET),\n",
        "]\n",
        "missing = [k for k, v in REQUIRED if not v]\n",
        "if missing:\n",
        "    raise SystemExit(f\"Missing required env vars: {', '.join(missing)}\")\n",
        "\n",
        "boto_config = boto3.session.Config(\n",
        "    retries={\"max_attempts\": 5, \"mode\": \"adaptive\"},\n",
        "    signature_version=\"s3v4\",\n",
        "    max_pool_connections=max(10, MAX_CONCURRENCY * 4),\n",
        "    connect_timeout=60,\n",
        "    read_timeout=300,\n",
        ")\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=SUPABASE_S3_ENDPOINT,\n",
        "    aws_access_key_id=SUPABASE_S3_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=SUPABASE_S3_SECRET_ACCESS_KEY,\n",
        "    region_name=SUPABASE_S3_REGION,\n",
        "    config=boto_config,\n",
        ")\n",
        "transfer_cfg = TransferConfig(\n",
        "    multipart_threshold=5 * 1024 * 1024,\n",
        "    multipart_chunksize=8 * 1024 * 1024,\n",
        "    max_concurrency=min(MAX_CONCURRENCY, 10),\n",
        "    use_threads=True,\n",
        ")\n",
        "\n",
        "DDL = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS metadata (\n",
        "  id BIGSERIAL PRIMARY KEY,\n",
        "  social_group TEXT NOT NULL,\n",
        "  date DATE NOT NULL,                -- first day of month\n",
        "  file_path TEXT NOT NULL,\n",
        "  num_rows BIGINT NOT NULL,\n",
        "  file_format TEXT,                  -- 'parquet' | 'csv' | 'csv.gz'\n",
        "  size_bytes BIGINT,\n",
        "  row_groups INT,\n",
        "  ts_min TIMESTAMPTZ,\n",
        "  ts_max TIMESTAMPTZ,\n",
        "  year  INT GENERATED ALWAYS AS (EXTRACT(YEAR  FROM date)::int) STORED,\n",
        "  month INT GENERATED ALWAYS AS (EXTRACT(MONTH FROM date)::int) STORED,\n",
        "  CONSTRAINT file_format_chk CHECK (file_format IN ('parquet','csv','csv.gz'))\n",
        ");\n",
        "CREATE UNIQUE INDEX IF NOT EXISTS uniq_metadata_row\n",
        "  ON metadata (social_group, date, file_path);\n",
        "CREATE INDEX IF NOT EXISTS idx_metadata_group_date\n",
        "  ON metadata (social_group, date);\n",
        "CREATE INDEX IF NOT EXISTS idx_metadata_group_year_month\n",
        "  ON metadata (social_group, year, month);\n",
        "\"\"\"\n",
        "\n",
        "async def create_db_pool(min_size: int = 1, max_size: int = 8) -> asyncpg.Pool:\n",
        "    pool = await asyncpg.create_pool(\n",
        "        SUPABASE_DB_URL,\n",
        "        min_size=min_size,\n",
        "        max_size=max_size,\n",
        "        timeout=30,\n",
        "    )\n",
        "    async with pool.acquire() as conn:\n",
        "        await conn.execute(DDL)\n",
        "    return pool\n",
        "\n",
        "async def upsert_metadata_with_retry(\n",
        "    pool: asyncpg.Pool,\n",
        "    group: str,\n",
        "    num_rows: int,\n",
        "    object_key: str,\n",
        "    file_date: datetime.date,\n",
        "    *,\n",
        "    file_format: str = \"parquet\",\n",
        "    size_bytes: Optional[int] = None,\n",
        "    row_groups: Optional[int] = None,\n",
        "    ts_min: Optional[datetime] = None,\n",
        "    ts_max: Optional[datetime] = None,\n",
        "    max_attempts: int = 5,\n",
        ") -> None:\n",
        "\n",
        "    last_err = None\n",
        "    for attempt in range(1, max_attempts + 1):\n",
        "        try:\n",
        "            async with pool.acquire() as conn:\n",
        "                await conn.execute(\n",
        "                    \"\"\"\n",
        "                    INSERT INTO metadata (social_group, num_rows, file_path, date,\n",
        "                                           file_format, size_bytes, row_groups, ts_min, ts_max)\n",
        "                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)\n",
        "                    ON CONFLICT (social_group, date, file_path)\n",
        "                    DO UPDATE SET\n",
        "                      num_rows   = EXCLUDED.num_rows,\n",
        "                      file_format= EXCLUDED.file_format,\n",
        "                      size_bytes = EXCLUDED.size_bytes,\n",
        "                      row_groups = EXCLUDED.row_groups,\n",
        "                      ts_min     = COALESCE(EXCLUDED.ts_min, metadata.ts_min),\n",
        "                      ts_max     = COALESCE(EXCLUDED.ts_max, metadata.ts_max)\n",
        "                    \"\"\",\n",
        "                    group, num_rows, object_key, file_date,\n",
        "                    file_format, size_bytes, row_groups, ts_min, ts_max\n",
        "                )\n",
        "            return\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            await asyncio.sleep(min(2 ** attempt, 10))\n",
        "    raise last_err\n",
        "\n",
        "READ_CSV_KW = dict(\n",
        "    usecols=[\n",
        "        \"id\", \"parent id\", \"text\", \"author\", \"time\",\n",
        "        \"subreddit\", \"score\", \"matched patterns\", \"source_row\"\n",
        "    ],\n",
        "    dtype={\n",
        "        \"score\": \"Int32\",\n",
        "        \"source_row\": \"Int32\",\n",
        "        \"id\": \"string\",\n",
        "        \"parent id\": \"string\",\n",
        "        \"text\": \"string\",\n",
        "        \"author\": \"string\",\n",
        "        \"time\": \"string\",\n",
        "        \"subreddit\": \"string\",\n",
        "        \"matched patterns\": \"string\",\n",
        "    },\n",
        "    parse_dates=False,\n",
        "    engine=\"c\",\n",
        "    low_memory=False,\n",
        ")\n",
        "\n",
        "FILENAME_MONTH_RE = re.compile(r\"(\\d{4}-\\d{2})\")\n",
        "\n",
        "def deduce_file_month_date(filename: str) -> datetime.date:\n",
        "    base = os.path.basename(filename)\n",
        "    m = FILENAME_MONTH_RE.search(base)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Could not parse YYYY-MM from filename '{filename}'\")\n",
        "    return datetime.strptime(m.group(1) + \"-01\", \"%Y-%m-%d\").date()\n",
        "\n",
        "def infer_social_group(path: Path) -> str:\n",
        "    return path.parent.name.strip() or \"ungrouped\"\n",
        "\n",
        "def parquet_object_key(group: str, filename: str, part_idx: int, sub_idx: Optional[int] = None) -> str:\n",
        "    base = os.path.splitext(os.path.basename(filename))[0]\n",
        "    if sub_idx is None:\n",
        "        return f\"{group}/{base}_part{part_idx}.parquet\"\n",
        "    else:\n",
        "        return f\"{group}/{base}_part{part_idx}_{sub_idx}.parquet\"\n",
        "\n",
        "def df_to_parquet_bytes(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    row_group_rows: int = ROW_GROUP_ROWS,\n",
        "    compression: str = \"zstd\",\n",
        ") -> bytes:\n",
        "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
        "    bio = io.BytesIO()\n",
        "    pq.write_table(\n",
        "        table,\n",
        "        where=bio,\n",
        "        compression=compression,\n",
        "        use_dictionary=True,\n",
        "        row_group_size=row_group_rows,\n",
        "        coerce_timestamps=\"us\",\n",
        "        flavor=\"spark\",\n",
        "    )\n",
        "    return bio.getvalue()\n",
        "\n",
        "def parquet_bytes_meta(parquet_bytes: bytes) -> Tuple[int, Optional[int]]:\n",
        "    size = len(parquet_bytes)\n",
        "    try:\n",
        "        pf = pq.ParquetFile(io.BytesIO(parquet_bytes))\n",
        "        return size, pf.metadata.num_row_groups\n",
        "    except Exception:\n",
        "        return size, None\n",
        "\n",
        "def split_df_to_parquet_parts_by_size(\n",
        "    df: pd.DataFrame,\n",
        "    max_bytes: int,\n",
        "    *,\n",
        "    row_group_rows: int = ROW_GROUP_ROWS,\n",
        ") -> List[bytes]:\n",
        "    parquet_bytes = df_to_parquet_bytes(df, row_group_rows=row_group_rows)\n",
        "    if len(parquet_bytes) <= max_bytes or len(df) <= 1:\n",
        "        return [parquet_bytes]\n",
        "\n",
        "    mid = len(df) // 2\n",
        "    left = split_df_to_parquet_parts_by_size(df.iloc[:mid], max_bytes, row_group_rows=row_group_rows)\n",
        "    right = split_df_to_parquet_parts_by_size(df.iloc[mid:], max_bytes, row_group_rows=row_group_rows)\n",
        "    return left + right\n",
        "\n",
        "def extract_ts_min_max(df: pd.DataFrame) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
        "    if \"time\" not in df.columns:\n",
        "        return None, None\n",
        "    try:\n",
        "        dt = pd.to_datetime(df[\"time\"], errors=\"coerce\", utc=True)\n",
        "        if dt.notna().any():\n",
        "            return dt.min().to_pydatetime(), dt.max().to_pydatetime()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "async def upload_parquet_and_record(\n",
        "    pool: asyncpg.Pool,\n",
        "    group: str,\n",
        "    src_filename: str,\n",
        "    part_idx: int,\n",
        "    sub_idx: Optional[int],\n",
        "    parquet_bytes: bytes,\n",
        "    rows: int,\n",
        "    file_date: datetime.date,\n",
        "    ts_bounds: Tuple[Optional[datetime], Optional[datetime]],\n",
        "    row_groups: Optional[int],\n",
        "    loop: asyncio.AbstractEventLoop,\n",
        "    executor: concurrent.futures.Executor,\n",
        ") -> Tuple[str, int]:\n",
        "    object_key = parquet_object_key(group, src_filename, part_idx, sub_idx)\n",
        "    size_mb = len(parquet_bytes) / (1024 * 1024)\n",
        "    print(f\"[UPLOAD] {object_key} | {rows} rows | {size_mb:.2f} MB | row_groups={row_groups}\")\n",
        "\n",
        "    def _upload():\n",
        "        s3_client.upload_fileobj(\n",
        "            Fileobj=io.BytesIO(parquet_bytes),\n",
        "            Bucket=SUPABASE_BUCKET,\n",
        "            Key=object_key,\n",
        "            ExtraArgs={\n",
        "                \"ContentType\": \"application/octet-stream\",\n",
        "                \"Metadata\": {\n",
        "                    \"rows\": str(rows),\n",
        "                    \"date\": str(file_date),\n",
        "                    \"group\": group,\n",
        "                    \"format\": \"parquet\",\n",
        "                },\n",
        "            },\n",
        "            Config=transfer_cfg,\n",
        "        )\n",
        "\n",
        "    await loop.run_in_executor(executor, _upload)\n",
        "\n",
        "    size_bytes = len(parquet_bytes)\n",
        "    ts_min, ts_max = ts_bounds\n",
        "\n",
        "    await upsert_metadata_with_retry(\n",
        "        pool,\n",
        "        group,\n",
        "        rows,\n",
        "        object_key,\n",
        "        file_date,\n",
        "        file_format=\"parquet\",\n",
        "        size_bytes=size_bytes,\n",
        "        row_groups=row_groups,\n",
        "        ts_min=ts_min,\n",
        "        ts_max=ts_max,\n",
        "    )\n",
        "    return object_key, rows\n",
        "\n",
        "async def download_csv_from_s3(s3_key: str) -> str:\n",
        "    \"\"\"Download CSV file from S3 and return local file path\"\"\"\n",
        "    local_path = f\"/tmp/{os.path.basename(s3_key)}\"\n",
        "    try:\n",
        "        s3_client.download_file(SUPABASE_BUCKET, s3_key, local_path)\n",
        "        return local_path\n",
        "    except ClientError as e:\n",
        "        print(f\"[ERR] Failed to download {s3_key}: {e}\")\n",
        "        raise\n",
        "\n",
        "async def process_single_file(\n",
        "    pool: asyncpg.Pool,\n",
        "    loop: asyncio.AbstractEventLoop,\n",
        "    executor: concurrent.futures.Executor,\n",
        "    s3_key: str,\n",
        "    group_override: Optional[str],\n",
        "    CHUNK_ROWS: int,\n",
        "    MAX_OBJECT_BYTES: int,\n",
        "    sem: asyncio.Semaphore,\n",
        ") -> int:\n",
        "    \"\"\"Process a single CSV file from Supabase Storage\"\"\"\n",
        "    try:\n",
        "        # Download CSV file from S3\n",
        "        local_file_path = await loop.run_in_executor(executor, download_csv_from_s3, s3_key)\n",
        "        file_path = Path(local_file_path)\n",
        "\n",
        "        if not file_path.exists() or not file_path.is_file():\n",
        "            print(f\"[WARN] Skipping (missing/not a file): {file_path}\")\n",
        "            return 0\n",
        "\n",
        "        try:\n",
        "            file_date = deduce_file_month_date(s3_key)\n",
        "        except ValueError as e:\n",
        "            print(f\"[WARN] {e} (file: {s3_key})\")\n",
        "            return 0\n",
        "\n",
        "        group = group_override or os.path.dirname(s3_key)\n",
        "        part_idx = 0\n",
        "        file_total = 0\n",
        "        tasks: List[asyncio.Task] = []\n",
        "\n",
        "        print(f\"[INFO] Processing: {s3_key}  group={group}  date={file_date}  \"\n",
        "              f\"chunk_rows={CHUNK_ROWS}  max_object={MAX_OBJECT_MB}MB parquet\")\n",
        "\n",
        "        try:\n",
        "            for chunk in pd.read_csv(str(file_path), chunksize=CHUNK_ROWS, **READ_CSV_KW):\n",
        "                part_idx += 1\n",
        "\n",
        "                ts_bounds = extract_ts_min_max(chunk)\n",
        "\n",
        "                parquet_parts = split_df_to_parquet_parts_by_size(\n",
        "                    chunk,\n",
        "                    MAX_OBJECT_BYTES,\n",
        "                    row_group_rows=ROW_GROUP_ROWS,\n",
        "                )\n",
        "                print(f\"[SPLIT] {os.path.basename(s3_key)} part {part_idx}: {len(parquet_parts)} parquet sub-parts\")\n",
        "\n",
        "                total_bytes = sum(len(b) for b in parquet_parts)\n",
        "                total_rows = len(chunk)\n",
        "\n",
        "                def est_rows_for_blob(blob_len: int) -> int:\n",
        "                    if total_bytes > 0:\n",
        "                        return max(1, round(total_rows * (blob_len / total_bytes)))\n",
        "                    return max(1, total_rows // max(1, len(parquet_parts)))\n",
        "\n",
        "                sub_idx = 0\n",
        "                for blob in parquet_parts:\n",
        "                    sub_idx += 1\n",
        "                    size_bytes, rg = parquet_bytes_meta(blob)\n",
        "                    rows_est = est_rows_for_blob(size_bytes)\n",
        "\n",
        "                    await sem.acquire()\n",
        "                    async def _do_upload(\n",
        "                        parquet_bytes=blob,\n",
        "                        rows=rows_est,\n",
        "                        pidx=part_idx,\n",
        "                        sidx=sub_idx,\n",
        "                        tsb=ts_bounds,\n",
        "                        row_groups=rg,\n",
        "                    ):\n",
        "                        nonlocal file_total\n",
        "                        try:\n",
        "                            key, r = await upload_parquet_and_record(\n",
        "                                pool, group, s3_key, pidx, sidx,\n",
        "                                parquet_bytes, rows, file_date, tsb, row_groups,\n",
        "                                loop, executor\n",
        "                            )\n",
        "                            file_total += r\n",
        "                            print(f\"[OK] {key}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"[ERR] Upload/record failed ({os.path.basename(s3_key)} part {pidx}_{sidx}): {e}\")\n",
        "                        finally:\n",
        "                            sem.release()\n",
        "\n",
        "                    tasks.append(asyncio.create_task(_do_upload()))\n",
        "            if tasks:\n",
        "                await asyncio.gather(*tasks)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERR] Failed to read {file_path}: {e}\")\n",
        "            return file_total\n",
        "        finally:\n",
        "            # Clean up local file\n",
        "            try:\n",
        "                file_path.unlink()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(f\"[DONE] {os.path.basename(s3_key)} ‚Üí ~{file_total:,} rows uploaded (estimated for split parts)\")\n",
        "        return file_total\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERR] Failed to process {s3_key}: {e}\")\n",
        "        return 0\n",
        "\n",
        "async def process_files_by_s3_keys(\n",
        "    S3_KEYS: List[str],\n",
        "    SINGLE_GROUP: Optional[str] = None,\n",
        "    CHUNK_ROWS: int = CHUNK_ROWS,\n",
        "    MAX_CONCURRENCY: int = MAX_CONCURRENCY,\n",
        "    MAX_OBJECT_BYTES: int = MAX_OBJECT_BYTES,\n",
        "):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    pool = await create_db_pool(min_size=1, max_size=max(4, MAX_CONCURRENCY * 2))\n",
        "\n",
        "    executor = concurrent.futures.ThreadPoolExecutor(\n",
        "        max_workers=max(2, min(os.cpu_count() or 4, 16))\n",
        "    )\n",
        "    pool_cap = boto_config.max_pool_connections or 10\n",
        "    eff_conc = min(MAX_CONCURRENCY, max(1, pool_cap // 2))\n",
        "    sem = asyncio.Semaphore(eff_conc)\n",
        "\n",
        "    print(f\"[CFG] MAX_CONCURRENCY requested={MAX_CONCURRENCY}  effective={eff_conc}  \"\n",
        "          f\"max_pool_connections={pool_cap}\")\n",
        "\n",
        "    total_uploaded = 0\n",
        "    try:\n",
        "        for s3_key in S3_KEYS:\n",
        "            rows = await process_single_file(\n",
        "                pool=pool,\n",
        "                loop=loop,\n",
        "                executor=executor,\n",
        "                s3_key=s3_key,\n",
        "                group_override=SINGLE_GROUP,\n",
        "                CHUNK_ROWS=CHUNK_ROWS,\n",
        "                MAX_OBJECT_BYTES=MAX_OBJECT_BYTES,\n",
        "                sem=sem,\n",
        "            )\n",
        "            total_uploaded += rows\n",
        "    finally:\n",
        "        await pool.close()\n",
        "        executor.shutdown(wait=True)\n",
        "\n",
        "    print(f\"\\nAll uploads complete. Total rows uploaded (approx across splits): {total_uploaded:,}\")\n",
        "\n",
        "async def process_files_by_path(\n",
        "    FILE_PATHS: List[str],\n",
        "    SINGLE_GROUP: Optional[str] = None,\n",
        "    CHUNK_ROWS: int = CHUNK_ROWS,\n",
        "    MAX_CONCURRENCY: int = MAX_CONCURRENCY,\n",
        "    MAX_OBJECT_BYTES: int = MAX_OBJECT_BYTES,\n",
        "):\n",
        "    \"\"\"Legacy function - kept for compatibility\"\"\"\n",
        "    loop = asyncio.get_running_loop()\n",
        "    pool = await create_db_pool(min_size=1, max_size=max(4, MAX_CONCURRENCY * 2))\n",
        "\n",
        "    executor = concurrent.futures.ThreadPoolExecutor(\n",
        "        max_workers=max(2, min(os.cpu_count() or 4, 16))\n",
        "    )\n",
        "    pool_cap = boto_config.max_pool_connections or 10\n",
        "    eff_conc = min(MAX_CONCURRENCY, max(1, pool_cap // 2))\n",
        "    sem = asyncio.Semaphore(eff_conc)\n",
        "\n",
        "    print(f\"[CFG] MAX_CONCURRENCY requested={MAX_CONCURRENCY}  effective={eff_conc}  \"\n",
        "          f\"max_pool_connections={pool_cap}\")\n",
        "\n",
        "    total_uploaded = 0\n",
        "    try:\n",
        "        for p in FILE_PATHS:\n",
        "            path_obj = Path(p).expanduser().resolve()\n",
        "            rows = await process_single_file(\n",
        "                pool=pool,\n",
        "                loop=loop,\n",
        "                executor=executor,\n",
        "                file_path=path_obj,\n",
        "                group_override=SINGLE_GROUP,\n",
        "                CHUNK_ROWS=CHUNK_ROWS,\n",
        "                MAX_OBJECT_BYTES=MAX_OBJECT_BYTES,\n",
        "                sem=sem,\n",
        "            )\n",
        "            total_uploaded += rows\n",
        "    finally:\n",
        "        await pool.close()\n",
        "        executor.shutdown(wait=True)\n",
        "\n",
        "    print(f\"\\nAll uploads complete. Total rows uploaded (approx across splits): {total_uploaded:,}\")\n",
        "\n",
        "async def repair_missing_metadata_by_prefix(\n",
        "    prefix: str,\n",
        "    group: str,\n",
        "    default_date: Optional[datetime.date] = None,\n",
        "):\n",
        "    pool = await create_db_pool(min_size=1, max_size=4)\n",
        "\n",
        "    keys: List[str] = []\n",
        "    token = None\n",
        "    while True:\n",
        "        kwargs = {\"Bucket\": SUPABASE_BUCKET, \"Prefix\": prefix}\n",
        "        if token:\n",
        "            kwargs[\"ContinuationToken\"] = token\n",
        "        resp = s3_client.list_objects_v2(**kwargs)\n",
        "        for c in resp.get(\"Contents\", []):\n",
        "            if c[\"Key\"].endswith(\".parquet\"):\n",
        "                keys.append(c[\"Key\"])\n",
        "        if resp.get(\"IsTruncated\"):\n",
        "            token = resp.get(\"NextContinuationToken\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"[REPAIR] Found {len(keys)} objects under {prefix}\")\n",
        "\n",
        "    async with pool.acquire() as conn:\n",
        "        existing = await conn.fetch(\n",
        "            \"SELECT file_path FROM metadata WHERE social_group=$1 AND file_path=ANY($2::text[])\",\n",
        "            group, keys\n",
        "        )\n",
        "    already = {r[\"file_path\"] for r in existing}\n",
        "    missing = [k for k in keys if k not in already]\n",
        "    print(f\"[REPAIR] Missing rows to insert: {len(missing)}\")\n",
        "\n",
        "    for k in missing:\n",
        "        head = s3_client.head_object(Bucket=SUPABASE_BUCKET, Key=k)\n",
        "        meta = head.get(\"Metadata\", {}) or {}\n",
        "        rows = int(meta.get(\"rows\", \"-1\"))\n",
        "        date_str = meta.get(\"date\")\n",
        "        fdate = datetime.strptime(date_str, \"%Y-%m-%d\").date() if date_str else (default_date or datetime.utcnow().date())\n",
        "        size_bytes = head.get(\"ContentLength\", None)\n",
        "\n",
        "        try:\n",
        "            await upsert_metadata_with_retry(\n",
        "                pool, group, rows, k, fdate,\n",
        "                file_format=\"parquet\",\n",
        "                size_bytes=size_bytes,\n",
        "                row_groups=None, ts_min=None, ts_max=None\n",
        "            )\n",
        "            print(f\"[REPAIR OK] {k} ({rows})\")\n",
        "        except Exception as e:\n",
        "            print(f\"[REPAIR ERR] {k}: {e}\")\n",
        "\n",
        "    await pool.close()\n",
        "\n",
        "\n",
        "# Get all CSV files from the target folders\n",
        "def get_all_csv_files_from_folders(folders):\n",
        "    \"\"\"Get all CSV files from specified folders in Supabase Storage\"\"\"\n",
        "    all_files = []\n",
        "    for folder in folders:\n",
        "        print(f\"üìÅ Scanning folder: {folder}\")\n",
        "        files = list_csv_files_in_folder(folder)\n",
        "        all_files.extend(files)\n",
        "        print(f\"   Found {len(files)} CSV files\")\n",
        "    return all_files\n",
        "\n",
        "# Get all CSV files from target folders\n",
        "all_csv_files = get_all_csv_files_from_folders(target_folders)\n",
        "print(f\"\\nüìä Total CSV files to process: {len(all_csv_files)}\")\n",
        "\n",
        "# Process files by folder (optional - you can process all at once or by folder)\n",
        "print(f\"\\nüöÄ Starting processing...\")\n",
        "print(f\"Processing {len(all_csv_files)} files from {len(target_folders)} folders\")\n",
        "\n",
        "# Process all files\n",
        "await process_files_by_s3_keys(\n",
        "    S3_KEYS=all_csv_files,\n",
        "    SINGLE_GROUP=None,  # Will use folder name as group\n",
        "    CHUNK_ROWS=80000,\n",
        "    MAX_CONCURRENCY=4,\n",
        "    MAX_OBJECT_BYTES=100*1024*1024,\n",
        ")\n",
        "\n",
        "# Optional: Repair missing metadata for specific folders\n",
        "# await repair_missing_metadata_by_prefix(\n",
        "#     prefix=\"race/RC_2023-03\",\n",
        "#     group=\"race\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLpaGyC8rm9_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
